{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mutUykZCI423"
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "train_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"train-clean-100\", download=True)\n",
    "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"test-clean\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pmccUUixJL5c"
   },
   "outputs": [],
   "source": [
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self):\n",
    "        char_map_str = \"\"\"\n",
    "        ' 0\n",
    "        <SPACE> 1\n",
    "        a 2\n",
    "        b 3\n",
    "        c 4\n",
    "        d 5\n",
    "        e 6\n",
    "        f 7\n",
    "        g 8\n",
    "        h 9\n",
    "        i 10\n",
    "        j 11\n",
    "        k 12\n",
    "        l 13\n",
    "        m 14\n",
    "        n 15\n",
    "        o 16\n",
    "        p 17\n",
    "        q 18\n",
    "        r 19\n",
    "        s 20\n",
    "        t 21\n",
    "        u 22\n",
    "        v 23\n",
    "        w 24\n",
    "        x 25\n",
    "        y 26\n",
    "        z 27\n",
    "        \"\"\"\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for line in char_map_str.strip().split('\\n'):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "        self.index_map[1] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['<SPACE>']\n",
    "            else:\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string).replace('<SPACE>', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-awhoaN4JVjx",
    "outputId": "40ca839b-5698-4e57-ba93-a34ced4db0ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate = 16000, n_mels = 128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param = 15),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param = 35)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "text_transform = TextTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "X-bvC588Jwce"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def data_preprocessing(data, data_type = \"train\"):\n",
    "  spectrograms = []\n",
    "  labels = []\n",
    "  input_lengths = []\n",
    "  label_lengths = []\n",
    "  for (waveform, _, utterance, _,_,_) in data:\n",
    "    if data_type == 'train':\n",
    "      spec = train_audio_transforms(waveform).squeeze(0).transpose(0,1)\n",
    "    else:\n",
    "      spec = valid_audio_transforms(waveform).squeeze(0).transpose(0,1)\n",
    "    spectrograms.append(spec)\n",
    "    label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "    labels.append(label)\n",
    "    input_lengths.append(spec.shape[0] // 2)\n",
    "    label_lengths.append(len(label))\n",
    "\n",
    "  spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first = True).unsqueeze(1).transpose(2,3)\n",
    "  labels = nn.utils.rnn.pad_sequence(labels, batch_first = True)\n",
    "\n",
    "  return spectrograms, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xK_xgZUpK2C9"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNLayerNorm(nn.Module):\n",
    "  def __init__(self, n_feats):\n",
    "    super(CNNLayerNorm, self).__init__()\n",
    "    self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "  def forward(self, x):\n",
    "    #(batch, channel, feature, time)\n",
    "    x = x.transpose(2,3).contiguous() #(batch, channel, time, feature)\n",
    "    x = self.layer_norm(x)\n",
    "    return x.transpose(2,3).contiguous() #(batch,channel,feature,time)\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "    super(ResidualCNN, self).__init__()\n",
    "    self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding = kernel // 2)\n",
    "    self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding = kernel // 2)\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "    self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "    self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "  def forward(self,x):\n",
    "    residual = x #(batch, channel, feature, time)\n",
    "    x = self.layer_norm1(x)\n",
    "    x = F.gelu(x)\n",
    "    x = self.dropout1(x)\n",
    "    x = self.cnn1(x)\n",
    "    x = self.layer_norm2(x)\n",
    "    x = F.gelu(x)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.cnn2(x)\n",
    "    x += residual\n",
    "    return x #(batch, channel, feature, time)\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "  def __init__(self,rnn_dim, hidden_size, dropout, batch_first):\n",
    "    super(BidirectionalGRU,self).__init__()\n",
    "\n",
    "    self.BiGRU = nn.GRU(\n",
    "        input_size = rnn_dim, hidden_size = hidden_size,\n",
    "        num_layers = 1, batch_first = batch_first, bidirectional = True\n",
    "    )\n",
    "    self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layer_norm(x)\n",
    "    x = F.gelu(x)\n",
    "    x, _ = self.BiGRU(x)\n",
    "    x = self.dropout(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yeUApzhdOSnw"
   },
   "outputs": [],
   "source": [
    "class SpeechRecognitionModel(nn.Module):\n",
    "  def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride = 2, dropout = 0.1):\n",
    "    super(SpeechRecognitionModel, self).__init__()\n",
    "\n",
    "    n_feats = n_feats // 2\n",
    "    self.cnn = nn.Conv2d(1, 32, 3, stride = stride, padding = 3 // 2) #cnn for extracting hierarchical features\n",
    "\n",
    "    #n residual cnn layers with filter size of 32\n",
    "    self.rescnn_layers = nn.Sequential(*[\n",
    "        ResidualCNN(32, 32, kernel = 3, stride = 1, dropout = dropout, n_feats = n_feats) for _ in range(n_cnn_layers)\n",
    "    ])\n",
    "    self.fully_connected = nn.Linear(n_feats * 32, rnn_dim)\n",
    "    self.birnn_layers = nn.Sequential(*[\n",
    "        BidirectionalGRU(rnn_dim = rnn_dim if i == 0 else rnn_dim * 2,\n",
    "                         hidden_size = rnn_dim,\n",
    "                         dropout = dropout,\n",
    "                         batch_first = i == 0) for i in range(n_rnn_layers)\n",
    "    ])\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Linear(rnn_dim * 2, rnn_dim),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(rnn_dim, n_class)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.cnn(x)\n",
    "    x = self.rescnn_layers(x)\n",
    "    sizes = x.size()\n",
    "    x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3]) #(batch,feature,time)\n",
    "    x = x.transpose(1,2) #(batch,time,feature)\n",
    "    x = self.fully_connected(x)\n",
    "    x = self.birnn_layers(x)\n",
    "    x = self.classifier(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktzzA63dQxzh",
    "outputId": "460987dc-ac4b-4c7c-c318-53fd0baeed0d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkevinv3796\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BsM6kWd3UFuL"
   },
   "outputs": [],
   "source": [
    "def greedy_decoder(output, labels, label_lengths, blank_label = 28, collapse_repeated = True):\n",
    "  arg_maxes = torch.argmax(output, dim = 2)\n",
    "  decodes = []\n",
    "  targets = []\n",
    "  for i, args in enumerate(arg_maxes):\n",
    "    decode = []\n",
    "    targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "    for j, index in enumerate(args):\n",
    "      if index != blank_label:\n",
    "        if collapse_repeated and j != 0 and index == args[j-1]:\n",
    "          continue #skip the character\n",
    "        decode.append(index.item())\n",
    "    decodes.append(text_transform.int_to_text(decode))\n",
    "  return decodes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Itb85H4tY0M8"
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "config = SimpleNamespace(\n",
    "  learning_rate=5e-4,\n",
    "  batch_size=32,\n",
    "  epochs=3,\n",
    "  n_cnn_layers=3,\n",
    "  n_rnn_layers = 5,\n",
    "  rnn_dim = 512,\n",
    "  n_class = 29,\n",
    "  n_feats = 128,\n",
    "  stride = 2,\n",
    "  dropout = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "o0Kl0mLTacEc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _levenshtein_distance(ref, hyp):\n",
    "  m = len(ref)\n",
    "  n = len(hyp)\n",
    "\n",
    "  if ref == hyp:\n",
    "    return 0\n",
    "  if m == 0:\n",
    "    return n\n",
    "  if n == 0:\n",
    "    return m\n",
    "\n",
    "  if m < n:\n",
    "    ref, hyp = hyp, ref\n",
    "    m, n = n, m\n",
    "\n",
    "  distance = np.zeros((2, n + 1), dtype = np.int32)\n",
    "\n",
    "  for j in range(0, n + 1):\n",
    "    distance[0][j] = j\n",
    "\n",
    "  for i in range(1, m + 1):\n",
    "    prev_row_idx = (i - 1) % 2\n",
    "    cur_row_idx = i % 2\n",
    "    distance[cur_row_idx][0] = i\n",
    "    for j in range(1, n + 1):\n",
    "      if ref[i-1] == hyp[j - 1]:\n",
    "        distance[cur_row_idx][j] = distance[prev_row_idx][j-1]\n",
    "      else:\n",
    "        s_num = distance[prev_row_idx][j - 1] + 1\n",
    "        i_num = distance[cur_row_idx][j - 1] + 1\n",
    "        d_num = distance[prev_row_idx][j] + 1\n",
    "        distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
    "\n",
    "  return distance[m % 2][n]\n",
    "\n",
    "def word_errors(reference, hypothesis, ignore_case = False, delimiter = ' '):\n",
    "  if ignore_case:\n",
    "    reference = reference.lower()\n",
    "    hypothesis = hypothesis.lower()\n",
    "\n",
    "  ref_words = reference.split(delimiter)\n",
    "  hyp_words = hypothesis.split(delimiter)\n",
    "\n",
    "  edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
    "  return float(edit_distance), len(ref_words)\n",
    "\n",
    "def char_error(reference, hypothesis, ignore_case = False, remove_space = False):\n",
    "  if ignore_case:\n",
    "    reference = reference.lower()\n",
    "    hypothesis = hypothesis.lower()\n",
    "\n",
    "  join_char = '' if remove_space else ' '\n",
    "\n",
    "  reference = join_char.join(filter(None, reference.split(' ')))\n",
    "  hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
    "\n",
    "  edit_distance = _levenshtein_distance(reference, hypothesis)\n",
    "  return float(edit_distance), len(reference)\n",
    "\n",
    "def wer(reference, hypothesis,ignore_case = False, delimiter = ' '):\n",
    "  edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case, delimiter)\n",
    "  if ref_len == 0:\n",
    "    raise ValueError(\"Reference's word number should be greater than 0\")\n",
    "\n",
    "  wer = edit_distance / ref_len\n",
    "  return wer\n",
    "\n",
    "def cer(reference, hypothesis, ignore_case = False, remove_space = False):\n",
    "  edit_distance, ref_len = char_error(reference, hypothesis, ignore_case, remove_space)\n",
    "\n",
    "  if ref_len == 0:\n",
    "    raise ValueError(\"Length of the reference should be greater than 0\")\n",
    "\n",
    "  cer = edit_distance / ref_len\n",
    "  return cer\n",
    "\n",
    "def avg_wer(wer_scores, combined_ref_len):\n",
    "  return float(sum(wer_scores)) / float(combined_ref_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2c1feCWqUy8e"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch):\n",
    "  model.train()\n",
    "  data_len = len(train_loader.dataset)\n",
    "  for batch_index, _data in tqdm(enumerate(train_loader)):\n",
    "    spectrograms, labels, input_lengths, label_lengths = _data\n",
    "    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(spectrograms) #(batch, time, n_class)\n",
    "    output = F.log_softmax(output, dim = 2)\n",
    "    output = output.transpose(0, 1) #(time, batch, n_class)\n",
    "\n",
    "    loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "    loss.backward()\n",
    "\n",
    "    wandb.log({\n",
    "        \"train/loss\": loss.item(),\n",
    "        \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "    })\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if batch_index % 100 == 0 or batch_index == data_len:\n",
    "      print(f\"Epoch: {epoch}, [{batch_index * len(spectrograms)}/{data_len} ({100. * batch_index / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3NERMlgRYQDX"
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, epoch):\n",
    "  print(\"\\nEvaluating...\")\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  test_cer, test_wer = [], []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, _data in tqdm(enumerate(test_loader)):\n",
    "      spectrograms, labels, input_lengths, label_lengths = _data\n",
    "      spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "      output = model(spectrograms) #(batch, time, n_class)\n",
    "      output = F.log_softmax(output, dim = 2)\n",
    "      output = output.transpose(0,1)  #(time, batch, n_class)\n",
    "\n",
    "      loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "      test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "      decoded_preds, decoded_targets = greedy_decoder(output.transpose(0,1), labels, label_lengths)\n",
    "      for j in range(len(decoded_preds)):\n",
    "        test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "        test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "  avg_cer = sum(test_cer) / len(test_cer)\n",
    "  avg_wer = sum(test_wer) / len(test_wer)\n",
    "  wandb.log({\n",
    "      \"valid/loss\": test_loss,\n",
    "      \"cer\": avg_cer,\n",
    "      \"wer\": avg_wer\n",
    "  })\n",
    "  print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "G7i8IfnrQ2Gf"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "criterion = nn.CTCLoss(blank = 28).to(device)\n",
    "\n",
    "train_url=\"train-clean-100\"\n",
    "test_url=\"test-clean\"\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = config.batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn = lambda x: data_preprocessing(x, 'train'),\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = config.batch_size,\n",
    "    shuffle = False,\n",
    "    collate_fn = lambda x: data_preprocessing(x, 'valid')\n",
    ")\n",
    "\n",
    "model = SpeechRecognitionModel(\n",
    "    config.n_cnn_layers, config.n_rnn_layers, config.rnn_dim, config.n_class, config.n_feats, config.stride, config.dropout\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr = config.learning_rate, steps_per_epoch = int(len(train_loader)), epochs = config.epochs, anneal_strategy = 'linear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.reset_max_memory_cached()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "56302c3de7f045f79ab6a83a47e08afb",
      "20910d41ce3141639c08e17d66b54e0c",
      "60f06f19a9244b2d90a4d17122538838",
      "dd78e8f5b0d4497ab4455723edfc7173",
      "15c1a1c5b8b9429cbac75e00e72eb9d7",
      "9e18ed4455af46c1803c0379ef1505fd",
      "fb09ea2b2263437f873428ccdb9ddfa9",
      "89f6f3fe9d60477b9faa7a2c187535ca"
     ]
    },
    "id": "OANStPtYrJ6A",
    "outputId": "7a255cc3-4894-4c6f-a1d0-6d1d0bbfbb90"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240411_133315-7v18iqb7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kevinv3796/automatic_speech_recognition/runs/7v18iqb7' target=\"_blank\">royal-butterfly-16</a></strong> to <a href='https://wandb.ai/kevinv3796/automatic_speech_recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kevinv3796/automatic_speech_recognition' target=\"_blank\">https://wandb.ai/kevinv3796/automatic_speech_recognition</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kevinv3796/automatic_speech_recognition/runs/7v18iqb7' target=\"_blank\">https://wandb.ai/kevinv3796/automatic_speech_recognition/runs/7v18iqb7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, [0/28539 (0%)]\tLoss: 0.943238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:45,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, [3200/28539 (11%)]\tLoss: 0.949081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [03:27,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, [6400/28539 (22%)]\tLoss: 1.010626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [05:10,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, [9600/28539 (34%)]\tLoss: 0.841963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [06:54,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, [12800/28539 (45%)]\tLoss: 0.875212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [08:36,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, [16000/28539 (56%)]\tLoss: 0.963474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [10:19,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, [19200/28539 (67%)]\tLoss: 1.030910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "701it [12:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, [22400/28539 (78%)]\tLoss: 0.917929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [13:46,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, [25600/28539 (90%)]\tLoss: 0.865717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "892it [15:20,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82it [07:09,  5.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.8426, Average CER: 0.256383 Average WER: 0.6837\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, [0/28539 (0%)]\tLoss: 0.864920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:42,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, [3200/28539 (11%)]\tLoss: 0.920299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [03:25,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, [6400/28539 (22%)]\tLoss: 0.799921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [05:08,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, [9600/28539 (34%)]\tLoss: 0.852760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [06:56,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, [12800/28539 (45%)]\tLoss: 0.821965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [08:39,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, [16000/28539 (56%)]\tLoss: 0.811105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [10:25,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, [19200/28539 (67%)]\tLoss: 0.784801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "701it [12:10,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, [22400/28539 (78%)]\tLoss: 0.830756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [13:53,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, [25600/28539 (90%)]\tLoss: 0.808473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "892it [15:30,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82it [07:20,  5.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.7131, Average CER: 0.217115 Average WER: 0.6143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, [0/28539 (0%)]\tLoss: 0.760739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:44,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, [3200/28539 (11%)]\tLoss: 0.762412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [03:33,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, [6400/28539 (22%)]\tLoss: 0.667481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [05:16,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, [9600/28539 (34%)]\tLoss: 0.750507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [07:05,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, [12800/28539 (45%)]\tLoss: 0.692839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [08:57,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, [16000/28539 (56%)]\tLoss: 0.718524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [10:42,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, [19200/28539 (67%)]\tLoss: 0.765869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "701it [12:26,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, [22400/28539 (78%)]\tLoss: 0.725599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [14:08,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, [25600/28539 (90%)]\tLoss: 0.681715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "892it [15:41,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82it [07:22,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.6577, Average CER: 0.200120 Average WER: 0.5734\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.027 MB of 0.027 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>cer</td><td>█▃▁</td></tr><tr><td>train/loss</td><td>▅▅▇▇▆▆▆▇██▆▇▆▆▆▆▄▄▆▅▄▄▅▄▇▄▁▂▃▂▄▃▁▁▃▂▄▂▂▂</td></tr><tr><td>valid/loss</td><td>█▃▁</td></tr><tr><td>wer</td><td>█▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cer</td><td>0.20012</td></tr><tr><td>train/loss</td><td>0.63896</td></tr><tr><td>valid/loss</td><td>0.65769</td></tr><tr><td>wer</td><td>0.57345</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">royal-butterfly-16</strong> at: <a href='https://wandb.ai/kevinv3796/automatic_speech_recognition/runs/7v18iqb7' target=\"_blank\">https://wandb.ai/kevinv3796/automatic_speech_recognition/runs/7v18iqb7</a><br/> View project at: <a href='https://wandb.ai/kevinv3796/automatic_speech_recognition' target=\"_blank\">https://wandb.ai/kevinv3796/automatic_speech_recognition</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240411_133315-7v18iqb7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project = \"automatic_speech_recognition\",\n",
    "                  job_type = 'train',\n",
    "                  config = config,\n",
    "                  tags = [\"speech_recognition\", \"audio\", \"cnn\", \"rnn\"]\n",
    "                  ):\n",
    "  for epoch in range(1, config.epochs + 1):\n",
    "    train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
    "    torch.save(model.state_dict(), f'model_checkpoint_{epoch}.pth')\n",
    "    torch.save(optimizer.state_dict(), f'optimizer_checkpoint_{epoch}.pth')\n",
    "    test(model, device, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample(model, device, test_loader, criterion):\n",
    "  model.eval()\n",
    "  val_samples = len(test_loader.dataset)\n",
    "  for i, _data in enumerate(test_loader):\n",
    "      if i == 5:\n",
    "          break\n",
    "      with torch.no_grad():\n",
    "          spectrograms, labels, input_lengths, label_lengths = _data\n",
    "          spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        \n",
    "          output = model(spectrograms) #(batch, time, n_class)\n",
    "          output = F.log_softmax(output, dim = 2)\n",
    "          output = output.transpose(0,1)  #(time, batch, n_class)\n",
    "        \n",
    "          decoded_preds, decoded_targets = greedy_decoder(output.transpose(0,1), labels, label_lengths)\n",
    "          print(f\"\\nDecoded speech {i}: \", decoded_preds[0])\n",
    "          print(f\"\\nGround labels {i}: \",decoded_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded speech 0:  he hoped there would be stoo her dinner turnips ond carit sand brused but tathos and fat buten peaces to be lataledoutand t tick pepered flouer facten sous\n",
      "\n",
      "Ground labels 0:  he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce\n",
      "\n",
      "Decoded speech 1:  he is calld as you know the ap possle of the indees\n",
      "\n",
      "Ground labels 1:  he is called as you know the apostle of the indies\n",
      "\n",
      "Decoded speech 2:  you wil fin me contieally speakng of for men titio whol bime turner and tin kcoret in almost the same turms\n",
      "\n",
      "Ground labels 2:  you will find me continually speaking of four men titian holbein turner and tintoret in almost the same terms\n",
      "\n",
      "Decoded speech 3:  is onmiap pensleout lined by ad would burn jearns in ellestration of the story of siky it is the interduction of siky af ter a her troubles in to heaven\n",
      "\n",
      "Ground labels 3:  it is only a pencil outline by edward burne jones in illustration of the story of psyche it is the introduction of psyche after all her troubles into heaven\n",
      "\n",
      "Decoded speech 4:  my har doff pled that thou in himin d ot ly a clot it never pearsedt with cristal eyees but the deffendons dof thot ple de ni and sas in hi ty faiire oparance lie\n",
      "\n",
      "Ground labels 4:  my heart doth plead that thou in him dost lie a closet never pierc'd with crystal eyes but the defendant doth that plea deny and says in him thy fair appearance lies\n"
     ]
    }
   ],
   "source": [
    "sample(model, device, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "As we can see, with 6 epochs of training, our model is already quite decent at transcription - we're at least getting words that sound quite similar to the ground labels, and this is only training on a subset of the librispeech dataset. \n",
    "\n",
    "Usually for ASR, CER and WER of around ~10-20% are recommended, but we are currently at only Average CER: 0.200120 Average WER: 0.5734."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model's state dictionary\n",
    "model.load_state_dict(torch.load('model_checkpoint.pth'))\n",
    "\n",
    "# Load the optimizer's state dictionary\n",
    "optimizer.load_state_dict(torch.load('optimizer_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2kWSBoDr7A9"
   },
   "outputs": [],
   "source": [
    "# get the code\n",
    "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
    "!cd ctcdecode && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9ujWkBGuUzX"
   },
   "outputs": [],
   "source": [
    "from ctcdecode import CTCBeamDecoder\n",
    "\n",
    "labels = \"\"\n",
    "\n",
    "beam_decoder = CTCBeamDecoder(\n",
    "    labels,\n",
    "    model_path=None,\n",
    "    alpha=0,\n",
    "    beta=0,\n",
    "    cutoff_top_n=40,\n",
    "    cutoff_prob=1.0,\n",
    "    beam_width=100,\n",
    "    num_processes=4,\n",
    "    blank_id=0,\n",
    "    log_probs_input=False\n",
    ")\n",
    "beam_results, beam_scores, timesteps, out_lens = decoder.decode(output) #output should be (batch_size, n_timesteps, n_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_beam(model, device, test_loader, criterion, epoch):\n",
    "  print(\"\\nEvaluating...\")\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  test_cer, test_wer = [], []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, _data in tqdm(enumerate(test_loader)[:5]):\n",
    "      spectrograms, labels, input_lengths, label_lengths = _data\n",
    "      spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "      output = model(spectrograms) #(batch, time, n_class)\n",
    "      output = F.log_softmax(output, dim = 2)\n",
    "      output = output.transpose(0,1)  #(time, batch, n_class)\n",
    "\n",
    "      loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "      test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "      decoded_preds, decoded_targets = greedy_decoder(output.transpose(0,1), labels, label_lengths)\n",
    "      for j in range(len(decoded_preds)):\n",
    "        test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "        test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "  avg_cer = sum(test_cer) / len(test_cer)\n",
    "  avg_wer = sum(test_wer) / len(test_wer)\n",
    "  \"\"\"wandb.log({\n",
    "      \"valid/loss\": test_loss,\n",
    "      \"cer\": avg_cer,\n",
    "      \"wer\": avg_wer\n",
    "  })\"\"\"\n",
    "  print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15c1a1c5b8b9429cbac75e00e72eb9d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20910d41ce3141639c08e17d66b54e0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15c1a1c5b8b9429cbac75e00e72eb9d7",
      "placeholder": "​",
      "style": "IPY_MODEL_9e18ed4455af46c1803c0379ef1505fd",
      "value": "0.013 MB of 0.013 MB uploaded\r"
     }
    },
    "56302c3de7f045f79ab6a83a47e08afb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_20910d41ce3141639c08e17d66b54e0c",
       "IPY_MODEL_60f06f19a9244b2d90a4d17122538838"
      ],
      "layout": "IPY_MODEL_dd78e8f5b0d4497ab4455723edfc7173"
     }
    },
    "60f06f19a9244b2d90a4d17122538838": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb09ea2b2263437f873428ccdb9ddfa9",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_89f6f3fe9d60477b9faa7a2c187535ca",
      "value": 1
     }
    },
    "89f6f3fe9d60477b9faa7a2c187535ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9e18ed4455af46c1803c0379ef1505fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dd78e8f5b0d4497ab4455723edfc7173": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb09ea2b2263437f873428ccdb9ddfa9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
